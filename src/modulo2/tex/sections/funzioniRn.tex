\section{Funzioni in $\mathbb{R}^n$}
Per definire le funzione abbiamo bisogni di definire gli intorni di un punto. In $\mathbb{R}^1$ abbiamo definito gli intorni sferici di un punto, mentre in $\mathbb{R}^n$ li definiamo come segue:
\dfn{
	Dato un \textbf{centro} $x \in \mathbb{R}^n$ e un \textbf{raggio} $r > 0$ poniamo 
	\begin{equation*}
		\mathcal{B}(x, r) = \{y \in \mathbb{R}^n \; | \; \lVert y - x \rVert < r\}
	\end{equation*}
	$\mathcal{B}$ si chiama \textbf{disco di centro $\mathbf{x}$ e raggio $\mathbf{r}$}
}
In pratica sarebbe l'insieme di punti che hanno distanza da $x$ minore o uguale a $r$. Se ci spostiamo un attimo in $\mathbb{R}^2$ e proviamo a prendere $x = (0, 0)$ e $r > 0$:
\begin{equation*}
	\mathcal{B}((0, 0), r) = \{(x, y) \in \mathbb{R}^2 \; | \; \lVert (x, y) - \underline{0} \rVert < r\}
\end{equation*}
Se si espande la definizione di norma si può notare che la condizione di appartenenza a questo insieme è:
\begin{equation*}
	\sqrt{x^2 + y^2} < r
\end{equation*}
Che corrisponde a tutti i punti contenuti in una circonferenza di raggio $r$ con centro nell'origine. Facendo invece lo stesso ragionamento ma con un punto $x = (x_0, y_0)$ generico si arriva alla condizione:
\begin{equation*}
	\sqrt{(x - x_0)^2 + (y - y_0)^2} < r
\end{equation*}
Dove anche questa volta l'equazione è una circonferenza di raggio $r$ ma con il centro spostato in $(x_0, y_0)$.

\dfn{
	Il \textbf{grafico} di $f: A \to \mathbb{R}^q$ con $A \subseteq \mathbb{R}^m$ è:
	\begin{equation*}
		\text{Graf}(f) = \{(x, f(x)) \in A\times \mathbb{R}^q\}
	\end{equation*}
}

\subsection{Funzioni scalari, affini, radiali e cilindriche}
Esistono vari tipi di funzioni in $\mathbb{R}^n$. In particolare quelle che hanno codomino $\mathbb{R}^1$ sono dette \textbf{funzioni scalari} in quanto appunto restituiscono uno scalare. Più precisamente:
\dfn{
	Una funzione scalare è del tipo $f: A \to \mathbb{R}$ dove $A \subseteq \mathbb{R}^n$.
}
Esempio: $f: \mathbb{R}^2 \to \mathbb{R}$ dove $f(x, y) = x^2 + y^2 = \lVert (x, y) \rVert ^2$. Se riprendiamo la definizione di grafico risulta che:
\begin{equation*}
	\text{Graf}(f) = \{(x, y, x^2 + y^2) \; | \; x, y \in \mathbb{R}^2\}
\end{equation*}
Si può notare che $(x, y, x^2 + y^2) \in \mathbb{R}^3$. Se lo intersechiamo con il piano $\pi = \{(0, y, z) \; | \; y, z \in \mathbb{R}\}$ il grafico diventa:
\begin{equation*}
	\text{Graf}(f) \medcap \pi = \{(0, y, y^2) \; | \; y \in \mathbb{R}^2\}
\end{equation*}
Che corrisponde all'insieme descritto dalle equazioni:
\begin{equation*}
	\begin{cases}
		x = 0\\
		z = y^2
	\end{cases}
\end{equation*}
Che non è altro che una parabola. %(DA FARE IL GRAFICO)
Se riscriviamo il nostro punto $(x, y)$ in coordinate polari e ricalcoliamo il valore della funzione:
\begin{equation*}
	f(x, y) = f(r\cos{x}, r\sin{x}) = r^2\cos^2(x) + r^2\sin^2(x) = r^2(\cos^2(x) + \sin^2(x)) = r^2
\end{equation*}
Prendendo ora $\pi_\theta = \{(r\cos{\theta}, r\sin\theta, z) | r \geq 0, z \in \mathbb{R}\}$ e lo interseco con il grafico di $f$:
\begin{equation*}
	\text{Graf}(f) \medcap \pi_\theta = \{(r\cos{\theta}, r\sin\theta, r^2) \; | \; r \geq 0\}
\end{equation*}
%(aggiungere foto GRAFICO)

Un ulteriore esempio sono le \textbf{funzionni radiali}:
\dfn{
	Una fuzione si dice \textbf{radiale} se è esprimibile nella forma:
	\begin{equation*}
		f: \mathbb{R}^2 \to \mathbb{R} \quad f(x, y) = g(\lVert (x, y) \rVert)
	\end{equation*}
	Con $g$ funzione \textit{opportuna}\footnote{In questo caso con funzione opportuna si intende una funzione sufficientemente regolare. Non siamo andati troppo nei dettagli in questo caso quindi riporto quello detto dal prof.} definita in $g:[0, +\infty[ \to \mathbb{R}$
}
Queste funzioni sono molto particolare perché il loro grafico è dato da rotazioni intorno all'asse $z$. Possiamo infatti considerare la funzione presa come esempio prima: 
\begin{equation*}
	f(x, y) = x^2 + y^2 = g(\lVert (x, y) \rVert) \quad \text{con} \;\; g(r) = r^2
\end{equation*}
Il grafico come abbiamo visto è una rotazione intorno all'asse $z$ di una parabola. L'equazione della parabola infatti risiede proprio in $g(r) = r^2$. È infatti questo il grafico che "ruota".

Se prendiamo un'altra funzione, tipo $g(r) = 1 -r$ e dove quindi:
\begin{equation*}
	f(x, y) = g(\lVert x, y \rVert) = 1 - \lVert x, y \rVert = 1 - \sqrt{x^2 + y^2}
\end{equation*}
Il grafico viso nella forma completa sembra complesso da disegnare, ma essendo questa una funzione radiale ci basta osservare la funzione $g$, disegnare il suo grafico e farlo ruotare intorno all'asse $z$. Il grafico risulta infatti un cono. %(FARE GRAFICO)

\dfn{
	Si definisce \textbf{funzione affine} una funzione nella forma:
	\begin{equation*}
		f: \mathbb{R}^2 \to \mathbb{R} \qquad f(x, y) = ax + by + c \;\; \text{con}\;\; a, b, c \in \mathbb{R}
	\end{equation*}
} 
Il grafico di queste funzioni risulta essere un piano in $\mathbb{R}^3$:
\begin{equation*}
	\text{Graf}(f) = \{(x, y, ax + by + c) | (x, y) \in \mathbb{R}^2\}
\end{equation*}
Prendiamo come esempio la funzione $f(x, y) = -y$: se la intersechiamo con il piano $\pi = \{(0, y, z) \; | \; y, z \in \mathbb{R}\}$ otteniamo un grafico del tipo:
\begin{equation*}
	\text{Graf}(f) \medcap \pi = \{(0, y, -y) | y \in \mathbb{R}\}
\end{equation*}
Che risulta un sempilce $z = -y$. Essendo che non dipende da $x$ possiamo intersecare tale grafico con quealsisi piano in cui la coordinata $x$ abbia un valore fissato. Ne cosegue che il grafico completo si ottiene traslando lungo l'asse $x$ la retta $z = -y$. %(FARE GRAFICO)

\dfn{
	Si definisce \textbf{funzione cilindrica} una funzione in due variabili che dipende da solo una delle due:
	\begin{equation*}
		f(x, y) = h(y) \qquad \text{oppure} \qquad f(x, y) = g(x)
	\end{equation*}
	Con $h$ e $g$ opportune.
}
Il grafico di queste funzioni si ottiene disegnando la funzione $z = h(y)$ (o nel caso $z = g(x)$) e successivamente traslando tale grafico sull'asse da cui non dipende la funzione. Per esempio la funzione presa prima in cosiderazione $f(x, y) = -y$ è una funzione cilindrica e si ottiene appunto tracciando la retta $z = -y$ e translado tale retta sull'asse $x$, cioè quello da cui non dipende la funzione. %(FARE GRAFICO)

Il fatto di traslare sull'asse da cui non dipende la funzione è semplicemente dato dal fatto che in $\mathbb{R}^3$ una grafico del tipo $\text{Graf}(f) = \{(x, y, h(y)) \;|\; x, y \in \mathbb{R}\}$ non impone nessun vincolo sul paramentro $x$, ne consegue che vale per tutti i suoi valori. Non siamo infatti constretti a disegnare tale funzione in $x = 0$, ma possiamo disegnarla per un qualsiasi valore di $x$ e poi traslare tutto il grafico lungo l'asse.

\subsection{Continuità funzioni scalari}
\dfn{
	Data una funzione $f: A \to \mathbb{R}$ con $A \subseteq \mathbb{R}^n$ e un punto $x_0 \in \mathbb{A}$, si dice che $f$ è \textbf{continua in $\mathbf{x_0}$} se:
	\begin{equation*}
		\forall (x_k)_{k \in \mathbb{N}} \subseteq A \quad (\text{cioè di punti in } A)
	\end{equation*}
	vale:
	\begin{equation*}
		x_k \xrightarrow[k \to +\infty]{} x_0 \implies f(x_k) \xrightarrow[k \to +\infty]{} f(x_0)
	\end{equation*}
	Oppure equivalentemente: $f$ è continua in $x_0 \in A$ se: 
	\begin{equation*}
		\forall \epsilon > 0, \exists \delta > 0 :
		\begin{cases}
			x \in A\\
			x \in \mathcal{B}(a, \delta)
		\end{cases}
		\implies | f(x) - f(x_0) | < \epsilon %(E' un valore assoluto o una norma??)
	\end{equation*}
	Dove $x \in B(x_0, \delta)$ è il disco di centro $x_0$ e raggio $\delta$.
}
\imp{
	Tutte le funzioni "elementari" sono \textbf{continue} nei loro domini.
}
Una funzione quindi del tipo
\begin{equation*}
	f(x, y) = \dfrac{\ln(y -x^2)}{\sqrt{1+\sin^2(xy)}}
\end{equation*}
è continua nel suo dominio, cioè:
\begin{equation*}
	\text{Dom}(f) = \{(x, y) \in \mathbb{R}^2 \;|\; y - x^2 > 0\}
\end{equation*}

\subsection{Insieme di livello}
\dfn{
	Dato $A \subseteq \mathbb{R}^n$, $f: A \to \mathbb{R}$ e $b \in \mathbb{R}$ si definisce \textbf{insieme di livello}:
	\begin{equation*}
		L_b = \{x \in A \;|\; f(x) = b\} = f^{-1}(b)
	\end{equation*}
	$f^{-1}(b)$ è la \textit{controimmagine} di $b$. Si noti che la controimmagine non richiede che la funzione sia invertibile perché appunto è l'insime di tutti i punti che presi dalla funzione restituiscono $b$. Se la funzione fosse invertibile questo insieme sarebbe composto da un elemento, cosa che però non è richiesta.
}
Gli insiemi di livello vengono largamente usati nelle cartine geografiche in quanto rappresenta una linea lungo la quale l'altidudine rimane costante. 

Facciamo un esempio in $\mathbb{R}^2$: Prendiamo la funzione $f: \mathbb{R}^2 \to \mathbb{R}$ definita come $f(x, y) = x^2 + y^2$. Proviamo a calcolare qualche insieme di livello:
\begin{enumerate}
	\item Se prendiamo come numero $b < 0$ notiamo che l'insieme di livello risulta vuoto:
		\begin{equation*}
			L_b = \{x \in \mathbb{R}^2 \;|\; x^2 + y^2 = b\} = \{x \in \mathbb{R}^2 \;|\; x^2 + y^2 < 0\} = \emptyset
		\end{equation*}
		Questo perché non esiste nessun vettore in $\mathbb{R}^2$ che soddisfi l'equazione $x^2 + y^2 < 0$.
	
	\item Se prendiamo invece come numero $b = 0$ l'insieme di livello risulta:
		\begin{equation*}
			L_b = \{x \in \mathbb{R}^2 \;|\; x^2 + y^2 = 0\} = \{(0, 0)\}
		\end{equation*}
		In quanto $x^2 + y^2 = 0 \iff (x, y) = (0, 0)$.

	\item Se prendiamo invece come numero $b > 0$ l'insieme di livello risulta:
		\begin{equation*}
			L_b = \{x \in \mathbb{R}^2 \;|\; x^2 + y^2 = b\}
		\end{equation*}
		L'equazione $x^2 + y^2 = b$ descrive una circonferenza di raggio $\sqrt{b}$. Possiamo osservare la funzione $f$ "dall'alto" e notare che su queste circonferenze la funzione è costante. È come se tagliassimo con un piano parallelo al piao $z = 0$ la funzione. %(FARE GRAFICO)
\end{enumerate}

\subsection{Derivabilità funzioni scalari}
In generale quando ci si trova in $\mathbb{R}^1$ si definisce la derivata di una funzione su un intervallo aperto del tipo $]a, b[$. Questo perché se si definisce la derivata su un intervallo chiuso si avrebbero dei problemi in quanto potrebbe esistere solo la derivata destra o sinistra in un punto. Definiamo quindi l'equivalente degli intervalli aperti in $\mathbb{R}^n$.
\dfn{
	Dato $A \subseteq \mathbb{R}^n$, dico che \textbf{$\mathbf{A}$ è aperto} se: 
	\begin{equation*}
		\forall x_0 \in A, \exists \epsilon > 0: \mathcal{B}(x_0, \epsilon) \subseteq A
	\end{equation*}
}
Se ci mettiamo in $\mathbb{R}^2$ un insieme di questo tipo è dato da un figura con i bordi tratteggiati, cioè dove tutti i punti sul bordo non appartengono all'insieme $A$. %(FARE FIGURA?)

\thm{
	\textbf{Individuazione degli intervalli aperti:} Data $f: \mathbb{R}^n \to \mathbb{R}$ continua, $\forall b \in \mathbb{R}$ l'insieme $\{x \in \mathbb{R}^n \;|\; f(x) > b\}$ è aperto. L'apertura è data dal fatto che l'insieme è definito tramite ugualianza stretta.
}

\subsubsection{Derivate parziali}
\dfn{
	Dato $A \subseteq \mathbb{R}^2$ aperto e sia $(x_0, y_0) \in A$. Si dice che $f$ \textbf{è derivabile rispetto a $\mathbf{x}$ in $\mathbf{(x_0, y_0)}$} se esiste \textbf{finito}:
	\begin{equation*}
		\pdv{f}{x}() (x_0, y_0) \vcentcolon = \lim_{t \to 0} = \dfrac{f(x_0 + t, y_0) - f(x_0, y_0)}{t} \in \mathbb{R}
	\end{equation*}
	Questo limite si indica con il simbolo di derivata parziale ($\partial$). Di seguito alcune notazioni equivalenti:
	\begin{equation*}
		\pdv{f}{x}() = \partial_x f = D_x f
	\end{equation*}
	Equivalentemente possiamo riscrivere il limite come:
	\begin{equation*}
		\pdv{f}{x}() (x_0, y_0) \vcentcolon = \lim_{x \to x_0} = \dfrac{f(x, y_0) - f(x_0, y_0)}{x - x_0} \in \mathbb{R}
	\end{equation*}
}
Vale ovviamente che la derivata rispetto a $y$ in $(x_0, y_0)$ risulta:
\begin{equation*}
		\pdv{f}{y}() (x_0, y_0) \vcentcolon = \lim_{t \to 0} = \dfrac{f(x_0, y_0 + t) - f(x_0, y_0)}{t} \in \mathbb{R}
\end{equation*}
Opppure equivalentemente:
\begin{equation*}
	\pdv{f}{y}() (x_0, y_0) \vcentcolon = \lim_{y \to y_0} = \dfrac{f(x_0, y) - f(x_0, y_0)}{y - y_0} \in \mathbb{R}
\end{equation*}

Esempio: prendiamo la funzione $f(x, y) = xy^2$. Le sue derivate parziali sono::
\begin{equation*}
	\pdv{f}{x}() (x, y) = y^2 \qquad \pdv{f}{y}() (x, y) = 2xy
\end{equation*}
L'idea per calcolare queste derivate è quella di considerarle come una funzioni normale (cioè come quelle viste in $\mathbb{R}^1$) e di considerare come variabile quella che bisogna derivare, mentre tutte le altre vengono considerate costanti. Nel primo caso infatti, dove abbiamo derivato rispetto alla $x$, ci è bastato considerare la $y$ costante. Quindi è come se avessimo fatto la derivata in $\mathbb{R}^1$ della semplice funzione:
\begin{equation*}
	f(x) = a^2 \cdot x \implies f'(x) = a^2
\end{equation*}
Dove però al posto della $a$ ci sarebbe la $y$. Nel secondo caso invece la derivata è rispetto alla $y$, quindi la $x$ viene considerata come semplice costante: $f(y) = a \cdot y^2 \implies f'(y) = 2ay$. Dove ovviamente al posto della $a$ ci sarebbe la $x$.\\

Se si vuole \textbf{generalizzare la definzione di derivata pariziale} è necessario riprendere la definzione di vettori coordinati (Sezione: \ref{sec_vettoriCoordinati}). In generale quindi data $f: \mathbb{R}^n \to \mathbb{R}$ e un punto $\overline{x} \in \mathbb{R}^n$ si definisce la \textit{i}-esima derivata parziale come:
\begin{equation*}
	\pdv{f}{x_i}()(\overline{x}) = \lim_{t \to 0} \dfrac{f(\overline{x} + t \cdot e_i) - f(\overline{x})}{t}
\end{equation*}

\subsubsection{Gradiente}
Le derivate parziali si possono aggregare in una funzione vettoriale (cioè che restituisce un vettore) chiamata gradiente:
\dfn{
	Data una funzione $f: A \in \mathbb{R}$ con $A \subseteq \mathbb{R}^n$, se esistono tutte le derivate pariziali di $f$ in ogni punto possiamo definire \textbf{il gradiente di $\mathbf{f}$} come:
	\begin{equation*}
		\nabla f(x_1, x_2, \cdots) = \left(\pdv{f}{x_1}()(x_1, x_2, \cdots), \pdv{f}{x_2}() (x_1, x_2, \cdots), \cdots  \right)
	\end{equation*}
	Nel caso particolare di $\mathbb{R}^2$:
	\begin{equation*}
		\nabla f(x, y) = \left(\pdv{f}{x}()(x, y), \pdv{f}{y}()(x, y)\right)
	\end{equation*}
	Il simbolo del gradiente ($\nabla$ si lege \textit{nabla}).
}
Esempio: data la funzione $f(x, y) = \sin(x + y^2)$ il suo gradiente risulta:
\begin{equation*}
	\nabla f (x, y) = (\cos(x + y^2), \cos(x + y^2) \cdot 2y)
\end{equation*}

\subsubsection{Derivailità e continuità}
In $\mathbb{R}^1$ vale la relazione:
\begin{equation*}
	f \text{ derivabile in } x \implies f \text{ continua in } x
\end{equation*}
Questa affermazione vale ache in generale? Cioè se per esempio prendiamo una funzione scalare con le derivate parziali definiti, allora questa funzione è continua? Proviamo a verificarlo\footnote{La seguente non è una dimostrazione di un teorema. Ho comunque voluto evidenziarla perché potrebbe risultare effettivamente importante}:
\pf{
	Vogliamo provare che presa una funzione scalare con dominio maggiore di $\mathbb{R}$ se esitono tutte le sue derivate parziali in un punto non vuol dire che quella funzione è derivabile. In particoalare data $f:\mathbb{R}^2 \to \mathbb{R}$:
	\begin{equation*}
		\exists \partial_x f(x, y), \partial_y f(x, y) \centernot \implies f \text{ continua}
	\end{equation*}

	Per dimostrare che qualcosa non è vero basta portare un controesempio: prendiamo la funzione definita:
	\begin{equation*}
		f(x, y) = 
		\begin{cases}
			\dfrac{xy}{x^2 + y^2} &\text{ se }(x, y) \neq (0, 0)\\
			0 &\text{ se }(x, y) = (0, 0)
		\end{cases}
	\end{equation*}
	Mostriamo quindi che esistono le derivate parziali in $(0, 0)$ ma la funzione è discontinua in $(0, 0)$.
	\begin{align*}
		\partial_x f(0, 0) = \lim_{t \to 0} \dfrac{f(t, 0) - f(0)}{t} = \lim_{t \to 0} \dfrac{\dfrac{t\cdot 0}{t^2 + 0} - 0}{t} = \lim_{t \to 0} \dfrac{0 - 0}{t} = 0
	\end{align*}
	Da notare che questa non è una forma indeterminata perché dalla definzione di limite $t \neq 0 \; \forall t \in \mathbb{R}$. Il numeratore invece è esattamente $0$. Per la derivata parziale rispetto a $y$ si fa esattamente la stessa cosa in quanto la funzione è simmetrica. Le derivate parziali quindi esistono:
	\begin{align*}
		\partial_x f(0, 0) = 0\\
		\partial_y f(0, 0) = 0\\
	\end{align*}
	Mostriamo ora che non è continua: prendiamo due successioni che tendono a $(0, 0)$:
	\begin{align*}
		(u_n, v_n) &= \left(0, \dfrac{1}{n}\right) \xrightarrow[n \to + \infty]{} (0, 0)\\
		(x_n, y_n) &= \left(\dfrac{1}{n}, \dfrac{1}{n}\right) \xrightarrow[n \to + \infty]{} (0, 0)
	\end{align*}
	Per essere continua devono valere i seguenti limiti:
	\begin{align*}
		f(u_n, v_n) \xrightarrow[n \to +\infty]{} f(0, 0) = 0\\
		f(x_n, y_n) \xrightarrow[n \to +\infty]{} f(0, 0) = 0
	\end{align*}
	Mostriamo che solo il primo vale:
	\begin{equation*}
		f(u_n, v_n) = \dfrac{0 \cdot \dfrac{1}{n}}{0^2 + \dfrac{1}{n^2}} \xrightarrow[n \to +\infty]{} 0
	\end{equation*}
	\begin{equation*}
		f(x_n, y_n) = \dfrac{\dfrac{1}{n} \cdot \dfrac{1}{n}}{\dfrac{1}{n^2} + \dfrac{1}{n^2}} = \dfrac{\dfrac{1}{n^2}}{2 \cdot \dfrac{1}{n^2}} = \dfrac{1}{2} \; \forall n
	\end{equation*}
	Ne consegue che la funzione è discontinua.
	\hfill Qed.
}
Abbiamo visto che non vale la relazione:
\begin{equation*}
	f \text{ derivabile in } x \implies f \text{ continua in } x
\end{equation*}
Quella che però vale è:
\begin{equation*}
	f \text{ differenziabile in } x \implies f \text{ continua in } x
\end{equation*}

\subsection{Differenziabilità}
In $\mathbb{R}^1$ preso un punto $x_0 \in \mathbb{R}$ vale che:
\begin{equation*}
	\exists f'(x_0) \in \mathbb{R} \iff \lim_{h \to 0} \dfrac{f(x_0 + h) - f(x_0)}{h} = f'(x_0)
\end{equation*}
Possiamo riscrivere il limite come segue (mantenendo sempre il \textit{se e solo se}):
\begin{align*}
	&\iff \lim_{h \to 0} \dfrac{f(x_0 + h) - f(x_0) - h \cdot f'(x_0)}{h} = 0\\[5pt]
	&\iff f(x_0 + h) - f(x_0) - f'(x_0) \cdot h = o(h)\\[5pt]
	&\iff f(x_0 + h) = f(x_0) + f'(x_0) \cdot h + o(h)
\end{align*}
Questa è la formula di Taylor di ordine $1$ per $h \to 0$. Se facciamo una sostituzione del tipo $x_0 + h = x$ possiamo riscrivere la formula per $x \to x_0$:
\begin{equation*}
	f(x) = f(x_0) + f'(x_0) (x - x_0) + o(x - x_0)
\end{equation*}

\dfn{
	\textbf{Notazioni per o-piccoli}: Sia $f:\mathbb{R}^2 \to \mathbb{R}$ si scrive che (con $p > 0$): 
	\begin{equation*}
		f(x, y) = o(\lVert (x, y) \rVert^p)
	\end{equation*}
	se: 
	\begin{equation*}
		\forall \epsilon > 0, \exists \delta > 0, \forall (x, y) : \lVert (x, y) \rVert < \delta \implies |f(x, y)| \leq \epsilon \cdot \lVert (x, y) \rVert^p
	\end{equation*}
	Se $(x, y) \neq (0, 0)$ si può riscrivere come:
	\begin{equation*}
		\cdots \implies \dfrac{f(x, y)}{\lVert (x, y) \rVert^p} \leq \epsilon
	\end{equation*}
	Equivalentemente\footnote{In realtà non è perfettamente equivalente perché il rapporto non sarebbe ben definito se $(x_n, y_n)$ assumesse il valore $(0, 0)$. In generale è quindi preferibile la prima definzione.}:
	\begin{equation*}
		\forall (x_n, y_n) \to (0, 0) \quad \text{ vale }\quad \lim_{n \to +\infty} \dfrac{f(x_n, y_n)}{\lVert (x_n, y_n) \rVert^p} = 0
	\end{equation*}
}
Esempio: verifichiamo che la funzione $f(x, y) = x^2$ è o-piccolo di $\lVert (x, y) \rVert$ per $(x, y) \to (0, 0)$. In questo caso $p = 1$. Deve quindi valere il limite:
\begin{equation*}
	\lim_{(x, y) \to (0, 0)} \dfrac{f(x, y)}{\lVert (x, y) \rVert^p} = \lim_{(x, y) \to (0, 0)} \dfrac{x^2}{\sqrt{x^2 + y^2}} = 0
\end{equation*}
Osserviamo che $|x| = \sqrt{x^2} \leq \sqrt{x^2 + y^2} = \lVert (x, y) \rVert$. Ne consegue che:
\begin{equation*}
	0 \leq \dfrac{x^2}{\lVert (x, y) \rVert} \leq \dfrac{\lVert (x, y) \rVert^2}{\lVert (x, y) \rVert} = \lVert (x, y) \rVert \xrightarrow[(x, y) \to (0, 0)]{} 0
\end{equation*}
Vale quindi il limite. Una cosa che non dimostriamo è che:
\begin{equation*}
	f(x, y) = \text{polinomio omogeneo dei grado p } \implies f(x, y) = o(\lVert (x, y) \rVert^p)
\end{equation*}
Un polinomio omogeneo è un polinomio che ha tutti i termini di grado esattamente $p$.

\dfn{
	\textbf{Funzione differenziabile:} Dato $A \subseteq \mathbb{R}^n$ aperto, $x_0 \in A$, $h \in \mathbb{R}^n$ e $f: A \to \mathbb{R}$ si dice che $f$ è \textbf{differenziabile in $x_0$} se:
	\begin{enumerate}
		\item $\exists \partial_1 f, \partial_2 f, \cdots, \partial_nf \in \mathbb{R}$ nel punto $x_0$
		\item Vale la formula:
			\begin{equation*}
				f(x_0 + h) = f(x_0) + \langle \nabla f(x_0), h\rangle +o(\lVert h \rVert) \quad h \to \underline{0} 
			\end{equation*}
	\end{enumerate}
	La 2 si può anche riscrivere con $x = x_0 + h$:
	\begin{equation*}
		f(x) = f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle + o(\lVert x - x_0 \lVert) \quad x \to x_0 
	\end{equation*}
}
Ovviamente la notazione con il prodotto euclideo si può espandere:
\begin{equation*}
	\langle \nabla f(x_0), h \rangle = \partial_1 f \cdot h_1 + \cdots + \partial_n f \cdot h_n = \sum_{k = 1}^n \partial_k f \cdot h_k\\
\end{equation*}
\imp{
	Se $f:\mathbb{R}^n \to \mathbb{R}$ è differenziabile in $x_0$ il polinomio
	\begin{equation*}
		T_1(x) = f(x_0) + \sum_{k = 1}^n \partial_k f(x_0) \cdot (x - x_0)\\
	\end{equation*}
	si chiama \textbf{polinomio di Taylor di grado 1 di punto inziale $\mathbf{x_0}$}. Inoltre in $\mathbb{R}^2$ il piano di equazione 
	\begin{equation*}
		z = T_1(x, y)
	\end{equation*}
	risulta essere il \textbf{piano tangente al grafico di $\textbf{f}$} nel punto $(x_0, y_0, f(x_0, y_0))$.
}

%(ESEMPIO CON GRAFICO?)

\thm{
	\textbf{Differenziale totale:} Data una funzione $f: \mathbb{R}^2 \to \mathbb{R}$ continua, se esistono le sue derivate parziali e sono continue in ogni punto allora $f$ è differenziabile in ogni punto. Cioè:
	\begin{equation*}
		\exists \partial_x f, \partial_y f \text{ continue } \forall x_0 \in \mathbb{R}^2 \implies f \text{ differenziabile } \forall y_0 \in \mathbb{R}^2
	\end{equation*}
}
\imp{
	\textbf{Tutte le funzioni elementari sono differenziabili} in quanto soddisfano la formula di Taylor.
}

Il seguente lemma è necessario alla dimostrazione che segue.
\mlem{
	\textbf{Lagrange per derivate parziali:} Se $f: \mathbb{R}^2 \to \mathbb{R}$ è continua e ha derivate parizali continue, allora:
	\begin{equation*}
		\forall (a, b) \in \mathbb{R}^2, \forall h \in \mathbb{R}, \; \exists \theta \in [0, 1] \,: \; f(a + h, b) - f(a, b) = \partial_x f(a + \theta h, b) h
	\end{equation*}
	Inoltre:
	\begin{equation*}
		\forall (a, b) \in \mathbb{R}^2, \forall k \in \mathbb{R}, \; \exists \theta \in [0, 1] \,: \; f(a, b + k) - f(a, b) = \partial_y f(a, b + \theta k) k
	\end{equation*}

	\textbf{Dimostrazione:}\\
	Definiamo una funzione ausiliaria $g: \mathbb{R} \to \mathbb{R}$ dove $g(x) = f(x, b) \; \forall x \in \mathbb{R}$. Applichiamo ora il teorema di Lagrange per $g$ nell'intervallo di estremi $[a, a + h]$:
	\begin{equation*}
		\exists \theta \in [0, 1]: g(a + h) - g(a) = g'(a + \theta h) (a + h - a)
	\end{equation*}
	Che per definzione di derivata parziale:
	\begin{equation*}
		g'(a + \theta h) \cdot h = \partial_x f(a + \theta h, b) \cdot h
	\end{equation*}
	\hfill Qed.

}
Questo lemma richiama il teorema di Lagrange visto per le funzioni definite in $\mathbb{R}^1$, solo che è scritto diversamente. Prendiamo il primo caso, quello con la derivata parziale in $x$ e proviamo a riscriverlo: Il punto $a + h$ è semplicemente un punto generico che quindi possiamo indicare con $A = a + h$. Il punto $a + \theta h$ invece, in quanto $\theta \in [0, 1]$, è semplicemente un altro modo di scrivere un punto nell'intervallo $[a, a + h]$. Possiamo quindi riscriverlo come $c \in [a, a+h]$, cioè per la sostituzione di prima $c \in [a, A]$. Possiamo quindi riscrivere l'enunciato come segue:
\begin{equation*}
	\forall (a, b) \in \mathbb{R}^2, \forall A \in \mathbb{R}, \; \exists c \in [a, A] : f(A, b) - f(a, b) = \partial_x f(c, b) \cdot (A - a)
\end{equation*}
In questo caso abbiamo riscritto $h = A - a$. Se inoltre assumiamo che $A \neq a \implies h \neq 0 $, quindi possiamo dividere per $h$:
\begin{equation*}
	\partial_x f(c, b) = \dfrac{f(A, b) - f(a, b)}{A - a}
\end{equation*}
Che se lo confrontiamo con il teorema di Lagrange per le funzioni ad una variabile ci assomiglia effettivamente molto:
\begin{equation*}
	f'(c) = \dfrac{f(b) - f(a)}{b - a}
\end{equation*}

Dimostriamo ora il teorema del differenziale totale:
\pf{
	Data $f: \mathbb{R}^2 \to \mathbb{R}$ continua di cui esistono le derivate parziali e sono anche loro continue devo dimostrare che $f$ è differenziale. Cioè per definzione di funzione differenziabile mi riduco a dimostrare:
	\begin{equation*}
		f(x_0 + h, y_0 + k) = f(x_0, y_0) + \langle \nabla f(x_0, y_0), (h, k) \rangle + o(\lVert (h, k) \rVert)
	\end{equation*}
	Per dimostrare una formula di questo tipo in pratica devo dimostrare che l'o-piccolo è corretto.

	\begin{equation*}
		f(x_0 + h, y_0 + k) - f(x_0, y_0) = \langle \nabla f(x_0, y_0), (h, k) \rangle + o(\lVert (h, k) \rVert)
	\end{equation*}
	Aggiungiamo e sottraiamo alla parte sinistra dell'uguaglianza il fattore $f(x_0 + h, y_0)$:
	\begin{equation*}
		f(x_0 + h, y_0 + k) - f(x_0 + h, y_0) + f(x_0 + h, y_0) - f(x_0, y_0)  = \langle \cdots \rangle + o(\cdots)
	\end{equation*}
	Possiamo raggruppare a due a due i termini identificandoli per comodità nel seguente modo:
	\begin{enumerate}
		\item $f(x_0 + h, y_0 + k) - f(x_0 + h, y_0)$

		\item $f(x_0 + h, y_0) - f(x_0, y_0)$
	\end{enumerate}
	Possiamo ora applicare Langrange per le derivate parziali (cioè il lemma enunciato sopra):\\
	Per 1:
	\begin{equation*}
		\exists \theta_1 \in [0, 1] \, : \; f(x_0 + h, y_0) - f(x_0, y_0) = \partial_x f(x_0 + \theta_1 h, y_0)h
	\end{equation*}
	Per 2:
	\begin{equation*}
		\exists \theta_2 \in [0, 1] \, : \; f(x_0 + h, y_0 + k) - f(x_0 + h, y_0) = \partial_y f(x_0 + h, y_0 + \theta_2 k)k
	\end{equation*}
	La 2 si può riscrivere aggiungendo e sottraendo il termine $\partial_x f(x_0, y_0) h$:
	\begin{align*}
		&\partial_x f(x_0 + \theta_1 h, y_0)h + \partial_x f(x_0, y_0) h - \partial_x f(x_0, y_0) h =\\[5pt]
		= &\; \partial_x f(x_0, y_0) h + [\partial_x f(x_0 + \theta_1 h, y_0) - \partial_x f(x_0, y_0) ]h
	\end{align*}
	Stesso discorso per la 1 aggiungendo e sottraendo il termine $\partial_y f(x_0, y_0) k$:
	\begin{equation*}
		\partial_y f(x_0, y_0) k + [\partial_y f(x_0 + h, y_0 + \theta_2 k) - \partial_y f(x_0, y_0)]k
	\end{equation*}
	I fattori che abbiamo aggiunto non sono scelti a caso ma bensì sono lo sviluppo del prodotto scalare euclideo presente nella formula inziale. Riscrviamo la formula iniziale:
	\begin{equation*}
		f(x_0 + h, y_0 + k) - f(x_0, y_0) = \langle \nabla f(x_0, y_0), (h, k) \rangle + o(\lVert (h, k) \rVert)\\
	\end{equation*}
	Sotituendo le espansioni di 1 e 2 ed espandendo il prodotto scalare euclideo:
	\begin{align*}
		&\partial_x f(x_0, y_0) h + [\partial_x f(x_0 + \theta_1 h, y_0) - \partial_x f(x_0, y_0) ]h +\\[2pt]
		&\partial_y f(x_0, y_0) k + [\partial_y f(x_0 + h, y_0 + \theta_2 k) - \partial_y f(x_0, y_0)]k \\[2pt]
		=& \partial_x f(x_0, y_0) h + \partial_y f(x_0, y_0) k + o(\lVert (h, k) \rVert)\\
	\end{align*}
	Semplificando i termini uguali da entrambe le parti dell'equazine diventa:
	\begin{equation*}
		[\partial_x f(x_0 + \theta_1 h, y_0) - \partial_x f(x_0, y_0) ]h + [\partial_y f(x_0 + h, y_0 + \theta_2 k) - \partial_y f(x_0, y_0)]k = o(\lVert (h, k) \rVert)
	\end{equation*}
	Ci basta quindi far vedere che tutto quello nelle parentesi quadre è effettivamente un o-piccolo della norma di $(h, k)$. Per la definzione di o-piccolo devo dimostrare che (vale la stessa cosa per l'altra parentesi quadra):
	\begin{equation*}
		\dfrac{[\partial_x f(x_0 + \theta_1 h, y_0) - \partial_x f(x_0, y_0) ]h}{\lVert (h, k) \rVert} \xrightarrow[(h, k) \to (0, 0)] {} 0
	\end{equation*}
	In quanto:
	\begin{equation*}
		\left | \dfrac{h}{\lVert (h, k) \rVert} \right | < \dfrac{\lVert (h, k) \rVert}{\lVert (h, k) \rVert} = 1
	\end{equation*}
	Mi basta quindi mostrare che:
	\begin{equation*}
		[\partial_x f(x_0 + \theta_1 h, y_0) - \partial_x f(x_0, y_0) ] \xrightarrow[(h, k) \to (0, 0)]{} 0
	\end{equation*}
	Prendiamo una successione $(h_n, k_n) \to (0, 0)$ e calcoliamo il limite:
	\begin{equation*}
		\lim_{n \to +\infty} \partial_x f(x_0 + \theta_1 h_n, y_0) - \partial_x f(x_0, y_0) =
	\end{equation*}
	Essedo per ipotesi $\partial_x f$ continua $(x_0 + \theta_1 h_n, y_0) \xrightarrow[h_n \to 0]{} (x_0, y_0)$:
	\begin{equation*}
		= \partial_x f(x_0, y_0) - \partial_x f(x_0, y_0) = 0
	\end{equation*}
	\hfill Qed.
}
